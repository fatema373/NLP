{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fatema373/NLP/blob/main/Text_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pyoqPFBRejr",
        "outputId": "13425a9e-bdf5-492e-ac18-3ec897351e7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sP1yeTJ1Vl3E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e9309626-4d06-422d-d595-88137b432a9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmJlDNodNGNo",
        "outputId": "49e6d585-b7bc-48ff-97aa-8c84e7ece0bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.22.4)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.10.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zp3RNyFQLKHC",
        "outputId": "fafcc324-a6c6-4107-eaf8-186cd0f7ed40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: country_list in /usr/local/lib/python3.10/dist-packages (1.0.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install country_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upkBrgWYVeEV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import pandas as pd\n",
        "import string\n",
        "import nltk\n",
        "import re\n",
        "import urllib,tarfile\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer,WordNetLemmatizer\n",
        "from country_list import countries_for_language\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "import gensim.downloader as api\n",
        "import gensim\n",
        "import numpy\n",
        "import sys\n",
        "from keras.models import *\n",
        "from keras.layers import *\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras import *\n",
        "import keras"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://wstyler.ucsd.edu/files/enronsentv1.tar.gz"
      ],
      "metadata": {
        "id": "b7-POLeMlMDr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "670e55b5-c585-4340-8b0b-ff1c5b538e3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-05-21 19:58:43--  http://wstyler.ucsd.edu/files/enronsentv1.tar.gz\n",
            "Resolving wstyler.ucsd.edu (wstyler.ucsd.edu)... 132.239.147.75\n",
            "Connecting to wstyler.ucsd.edu (wstyler.ucsd.edu)|132.239.147.75|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://wstyler.ucsd.edu/files/enronsentv1.tar.gz [following]\n",
            "--2023-05-21 19:58:43--  https://wstyler.ucsd.edu/files/enronsentv1.tar.gz\n",
            "Connecting to wstyler.ucsd.edu (wstyler.ucsd.edu)|132.239.147.75|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 26406491 (25M) [application/x-gzip]\n",
            "Saving to: ‘enronsentv1.tar.gz.1’\n",
            "\n",
            "enronsentv1.tar.gz. 100%[===================>]  25.18M  2.50MB/s    in 10s     \n",
            "\n",
            "2023-05-21 19:58:54 (2.49 MB/s) - ‘enronsentv1.tar.gz.1’ saved [26406491/26406491]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVic3QZsJTf8"
      },
      "outputs": [],
      "source": [
        "with tarfile.open(\"/content/enronsentv1.tar.gz\", 'r:gz') as tar:\n",
        "    tar.extractall()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YC8yxXlJjOVy"
      },
      "outputs": [],
      "source": [
        "# path = '/content/enronsent'\n",
        "# paths = list(set(os.listdir(path)))\n",
        "# df = pd.DataFrame()\n",
        "# df['docs']=[read_file(os.path.join(path, x)) for x in os.listdir(path) if 'enronsent' in x and '._enronsent' not in x]\n",
        "# df['file_name']= [int(x[-6:-4]) for x in os.listdir(path) if 'enronsent' in x and '._enronsent' not in x]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DataLoader:\n",
        "    def __init__(self,originpath):\n",
        "        self.originpath=originpath\n",
        "        self.df=pd.DataFrame()\n",
        "        # save the docs and their number\n",
        "        # the files is named as following enronsentXX, the XX is representing the no of doc\n",
        "        #  so we extract these two indecies and add them to be the df indexing in the future\n",
        "        self.df['origin_doc']=self.read_docs(self.originpath)\n",
        "        self.preprocessing()\n",
        "        self.df=self.padding(self.df,self.tokenizer)\n",
        "    def padding(df,tokenizer):\n",
        "        max_length = 15\n",
        "        def sequences(p):\n",
        "            return tokenizer.texts_to_sequences(p)\n",
        "        df['padding'] = df.clean_text.apply(lambda p: pad_sequences(sequences(p), maxlen=max_length, padding='post'))\n",
        "        return df\n",
        "    def read_docs(self,path):\n",
        "        docs=[]\n",
        "        for doc in os.listdir(path):\n",
        "            doc_path=os.path.join(path,doc)\n",
        "            with open(doc_path,encoding='utf-8',errors='ignore') as file:\n",
        "                docs.append(file.read().replace('\\n', ''))\n",
        "        return docs\n",
        "\n",
        "    def change_file_extension(path):\n",
        "        for file in os.listdir(path):\n",
        "            # print (file)\n",
        "            head, tail = os.path.splitext(file)\n",
        "            if not tail:\n",
        "                # print('fdf')\n",
        "                dst = os.path.join(path, head + '.txt')\n",
        "                src = os.path.join(path, head)\n",
        "                new_name = head + '.txt'\n",
        "                new_filepath = os.path.join(path, new_name)\n",
        "                os.rename(src, new_filepath)\n",
        "    def preprocessing(self):\n",
        "        # sentence_end_regex = r\"[.!?]\" #convert docs to sentences\n",
        "        # self.df['sentences']=self.df['origin_doc'].apply(lambda x : list(filter(None,list(re.split(sentence_end_regex, x)))))\n",
        "\n",
        "        pretext=[]\n",
        "        for doc in self.df['origin_doc']:# remove chars and puncs\n",
        "            punctuations = string.punctuation\n",
        "            pretext.append(doc.translate(str.maketrans('','',punctuations)))\n",
        "\n",
        "        self.df['pre_text']= pretext\n",
        "        punctuations = string.punctuation\n",
        "\n",
        "        stop_words = set(stopwords.words('english')) # remove stop words\n",
        "        pretext=[]\n",
        "        for doc in self.df.pre_text:\n",
        "            pretext.append(' '.join(word for word in doc.split() if word not in stop_words))\n",
        "        self.df['pre_text']= pretext\n",
        "        pretext=[]\n",
        "        for doc in  self.df['pre_text']:\n",
        "            doc = re.sub('[^a-zA-Z]',' ',doc)\n",
        "            doc = re.sub('\\s+',' ',doc)\n",
        "            pretext.append(doc)\n",
        "        self.df['pre_text']= pretext\n",
        "\n",
        "        pretext=[]\n",
        "        for doc in  self.df['pre_text']:\n",
        "            countries = dict(countries_for_language('en'))\n",
        "            countries_Vals = [x.lower() for x in list(countries.values())]\n",
        "            countries_Keys = [x.lower() for x in list(countries.keys())]\n",
        "            countries_to_be_removed=countries_Vals+countries_Keys\n",
        "            pretext.append(' '.join(word for word in doc.split() if word not in (countries_to_be_removed)))\n",
        "        self.df['pre_text']= pretext\n",
        "\n",
        "        months=['january','february','march','april', 'may','june','july','august','september','october','november','december']\n",
        "        pretext=[]\n",
        "        for doc in self.df['pre_text']:\n",
        "            pretext.append(' '.join(word for word in doc.split() if word not in (months)))\n",
        "        self.df['pre_text']= pretext\n",
        "        self.df.pre_text = df.pre_text.apply(lambda x : list(filter(None,x))) #remove nanas\n",
        "\n",
        "        X=[]\n",
        "        for text in df.pre_text:\n",
        "            X.extend(text)\n",
        "            tokenizer = Tokenizer()\n",
        "            tokenizer.fit_on_texts(X)\n",
        "        self.tokenizer=tokenizer\n",
        "    def get_data(self):\n",
        "        return self.df"
      ],
      "metadata": {
        "id": "0rndUGHVmnkX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/enronsent'\n",
        "dataLoader=DataLoader(path)"
      ],
      "metadata": {
        "id": "7OmiEQ08mbBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=dataLoader.df"
      ],
      "metadata": {
        "id": "Ea06SXys6EXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_6lUCsoL8e1"
      },
      "outputs": [],
      "source": [
        "model = gensim.models.KeyedVectors.load_word2vec_format('/content/drive/MyDrive/GoogleNews-vectors-negative300.bin.gz', binary=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VKatrvzJXaFr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a6e313d-e223-460b-f180-1baf17d624e3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "300"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "model.vector_size"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=dataLoader.tokenizer"
      ],
      "metadata": {
        "id": "EarvVz9LD7rI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_words = len(tokenizer.word_index) + 1\n",
        "embedding_dim = 300\n",
        "embedding_matrix = np.zeros((num_words, embedding_dim))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    if word in model:\n",
        "        embedding_matrix[i] = model[word]"
      ],
      "metadata": {
        "id": "WAPzzHKLhaQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train =np.array(df.padding.iloc[:40])\n",
        "validation = np.array(df.padding.iloc[40:])"
      ],
      "metadata": {
        "id": "o0nFrMJYkV9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "YBRYoZH9EiVm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train =np.array(df.padding.iloc[:11])\n",
        "validation = np.array(df.padding.iloc[11:])"
      ],
      "metadata": {
        "id": "YGyas0vjFAEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_batch(X_, n_steps, tokenizer):\n",
        "  X, y = [],[]\n",
        "  for i in range(len(X_)-n_steps-1):\n",
        "    words = tokenizer.sequences_to_texts([[i] for i in X_[i:i+n_steps]])\n",
        "    words = [word for word in words if len(word)>0]\n",
        "    if len(words)==0:\n",
        "      return [],[]\n",
        "    X.append(model[words])\n",
        "    y.append(model[tokenizer.sequences_to_texts([[X_[i+n_steps]]])])\n",
        "  X=np.array(X)\n",
        "  return X, np.array(y)"
      ],
      "metadata": {
        "id": "yxl2erJ5tcKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def timeStep_prep(data,timestep=1):\n",
        "    X = list()\n",
        "    y = list()\n",
        "    for i in range(timestep, data.shape[0]):\n",
        "        step = np.zeros((timestep, data.shape[0]-1))\n",
        "        for j in reversed(range(1,timestep+1)):\n",
        "            step[timestep-j] = data[i-j]\n",
        "        y.append(data[i])\n",
        "        X.append(step)\n",
        "    return np.array([np.array(X), np.array(y)])"
      ],
      "metadata": {
        "id": "9sWoKDFXoAVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # num_words = len(tokenizer.word_index) + 1\n",
        "# # embedding_dim = 300\n",
        "# # max_length=15\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "early_stop_LSTM = EarlyStopping(monitor='val_loss', patience=10, verbose=1, mode='min', restore_best_weights=True)\n",
        "\n",
        "# embedding_layer = Embedding(num_words, embedding_dim, weights=[embedding_matrix], input_length=max_length, trainable=False)\n",
        "LSTM_model = Sequential()\n",
        "LSTM_model.add(Input((4,300)))\n",
        "# LSTM_model.add(embedding_layer)\n",
        "LSTM_model.add(LSTM(4,activation=\"relu\",dropout=0.3))\n",
        "LSTM_model.add(BatchNormalization())\n",
        "LSTM_model.add(Dense(512,activation='relu'))\n",
        "LSTM_model.add(Dense(512,activation='relu'))\n",
        "LSTM_model.add(Dense(256,activation='relu'))\n",
        "LSTM_model.add(Dense(len(b),activation='softmax'))\n",
        "optimizer = keras.optimizers.Adam(lr=0.3)\n",
        "\n",
        "LSTM_model.compile(loss='categorical_crossentropy', optimizer=optimizer,metrics=['categorical_crossentropy'])\n",
        "\n",
        "LSTM_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8ad1ef2-033e-4ede-ed61-b93141fe8a1c",
        "id": "zA7nSYEBEegA"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (None, 4)                 4880      \n",
            "                                                                 \n",
            " batch_normalization (BatchN  (None, 4)                16        \n",
            " ormalization)                                                   \n",
            "                                                                 \n",
            " dense (Dense)               (None, 512)               2560      \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 512)               262656    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 256)               131328    \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 297569)            76475233  \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 76,876,673\n",
            "Trainable params: 76,876,665\n",
            "Non-trainable params: 8\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/optimizers/legacy/adam.py:117: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super().__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oCfsSTw9-vnM"
      },
      "outputs": [],
      "source": [
        "data=[create_batch(sc,4) for doc in train for sc in doc if len(create_batch(sc,4))==2 ]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.sequence import TimeseriesGenerator\n",
        "generator = TimeseriesGenerator(train, train, length=15, batch_size=4)"
      ],
      "metadata": {
        "id": "iUheW5JM0E6f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ut-l7NMY_-si"
      },
      "outputs": [],
      "source": [
        "generator[1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rrsIsyQbDQV7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}